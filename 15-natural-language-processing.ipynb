{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "<span style=\"font-size: 12px;\">By: Marisol Hernandez</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Natural Language Processing\n",
    "**Natural Language Processing (NLP)** is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The goal of NLP is to read, decipher, understand, and make sense of human languages in a way that is valuable.\n",
    "\n",
    "Specifically, NLP seeks to program computers to process or analyze large amounts of natural language data (such as written or spoken text) in such a way that a coherent interpretation or production of the language is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications and use cases\n",
    "Some common use cases of NLP include:\n",
    "\n",
    "1. **Tokenization**: The process of breaking down text into smaller units called *tokens*, which can be words, subwords, or characters. This is a fundamental step in many NLP tasks, enabling further analysis by simplifying the text into manageable pieces.\n",
    "\n",
    "2. **Syntactic Analysis**: Also known as *parsing*, involves analyzing the grammatical structure of a sentence. It identifies the roles of words and their relationships to each other, creating a parse tree.\n",
    "\n",
    "3. **Lemmatization and Stemming**: Lemmatization and stemming reduce words to their base or root forms. Stemming cuts words to their base form, while lemmatization uses a dictionary to convert words to their root form.\n",
    "\n",
    "4. **Named Entity Recognition (NER)**: Identifies and classifies proper names in text, such as names of people, organizations, locations, dates, and more.\n",
    "\n",
    "5. **Sentiment Analysis**: Determines the emotional tone behind a body of text. It identifies whether the text is positive, negative, or neutral.\n",
    "\n",
    "6. **Automatic Translation**: Involves translating text from one language to another using machine learning models.\n",
    "\n",
    "7. **Question Answering**: Question answering systems automatically respond to questions posed in natural language. These systems can retrieve answers from structured databases or unstructured text sources.\n",
    "\n",
    "8. **Natural Language Generation (NLG)**: Involves creating coherent and contextually relevant text from data. It is used to generate reports, summaries, and narratives.\n",
    "\n",
    "9. **Automatic Summarization**: Reduces a large body of text to a shorter version, preserving its most important information. This can be extractive (selecting key sentences) or abstractive (generating new sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "Tokenization is the process of breaking down text into smaller units called *tokens*. Tokens can be words, characters, numbers, symbols, or n-grams.\n",
    "\n",
    "The most common tokenization process is whitespace/ unigram tokenization. As shown in the example below the whole sentence is split into unigrams i.e `[“We”, \"love”, ”NLP”, ”!”]`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/nlp1.png\" alt=\"Alt text\" width=\"600\" height=\"300\">\n",
    "</p>\n",
    "\n",
    "There are a number of techniques to perform tokenization in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization Using Python’s `split()` function\n",
    "One of the simplest ways to achieve tokenization in Python is by using the `split()` function, which splits a string into a list of substrings based on a specified delimiter. If no delimiter is provided, the default is whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Tokens: ['Hello,', 'world!', 'Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'fun.', \"Let's\", 'learn', 'it', 'together.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Splits at space\n",
    "tokens = text.split()\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization Using Regular Expressions (RegEx)\n",
    "**Regular expressions (RegEx)** are sequences of characters that define a search pattern, primarily used for string pattern matching and manipulation. They are powerful tools for text processing and are widely used in programming, particularly for tasks like searching, replacing, and parsing text.\n",
    "\n",
    "**Basic Elements**:\n",
    "\n",
    "- `.`: Matches any single character except newline.\n",
    "\n",
    "- `\\w`: Matches any word character (letters, digits, and underscore).\n",
    "\n",
    "- `\\d`: Matches any digit.\n",
    "\n",
    "- `\\s`: Matches any whitespace character (spaces, tabs, newlines).\n",
    "\n",
    "- `[]`: Matches any single character within the brackets. For example, `[abc]` matches `'a'`, `'b'`, or `'c'`.\n",
    "\n",
    "- `^`: Matches the start of a string.\n",
    "\n",
    "- `$`: Matches the end of a string.\n",
    "\n",
    "- `*`: Matches zero or more repetitions of the preceding element.\n",
    "\n",
    "- `+`: Matches one or more repetitions of the preceding element.\n",
    "\n",
    "- `?`: Matches zero or one repetition of the preceding element.\n",
    "\n",
    "- `{n}`: Matches exactly n repetitions of the preceding element.\n",
    "\n",
    "- `|`: Acts as a logical OR between patterns.\n",
    "\n",
    "By using these elements and combinations of them, you can create complex patterns to match and manipulate text in versatile ways. In the provided code, `r\"[\\w]+\"` is used to find and extract all sequences of word characters, effectively tokenizing the text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Tokens: ['Hello', 'world', 'Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'fun', 'Let', 's', 'learn', 'it', 'together']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "tokens = re.findall(r\"[\\w]+\", text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenization Using NLTK\n",
    "NLTK contains a module called `tokenize()` which further classifies into two sub-categories:\n",
    "\n",
    "1. **Sentence tokenize**: We use the `sent_tokenize()` method to split a document or paragraph into sentences\n",
    "\n",
    "2. **Word tokenize**: We use the `word_tokenize()` method to split a sentence into tokens or words\n",
    "\n",
    "#### Installation\n",
    "First, ensure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn) (3.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, download the required NLTK data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform sentence and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Sentences: ['Hello, world!', 'Natural Language Processing with Python is fun.', \"Let's learn it together.\"] \n",
      "\n",
      "Words: ['Hello', ',', 'world', '!', 'Natural', 'Language', 'Processing', 'with', 'Python', 'is', 'fun', '.', 'Let', \"'s\", 'learn', 'it', 'together', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences, \"\\n\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Words:\", words, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Alternative Tokenization Techniques\n",
    "For more advanced and specialized tokenization techniques, you can explore the following resources:\n",
    "\n",
    "1. [Tokenization Using SpaCy](https://spacy.io/api/tokenizer)\n",
    "\n",
    "2. [Tokenization Using Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/t#ext/Tokenizer)\n",
    "\n",
    "3. [Tokenization Using Gensim](https://tedboy.github.io/nlps/generated/generated/gensim.utils.tokenize.html)\n",
    "\n",
    "4. [Tokenization Using PunktSentenceTokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stop Words Removal\n",
    "Stop words are common words that usually do not carry significant meaning and are often removed from text. \n",
    "\n",
    "For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/nlp2.png\" alt=\"Alt text\">\n",
    "</p>\n",
    "\n",
    "Generally, the most common words used in a text are `“the”`, `“is”`, `“in”`, `“for”`, `“where”`, `“when”`, `“to”`, `“at”` etc. A note here – we need to perform tokenization before removing any stopwords.\n",
    "\n",
    "### Why do we Need to Remove Stopwords?\n",
    "**Removing stopwords is not a hard and fast rule in NLP.** It depends upon the task that we are working on. For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text.\n",
    "\n",
    "However, in tasks like machine translation and text summarization, removing stopwords is not advisable.\n",
    "\n",
    "Here are a few key benefits of removing stopwords:\n",
    "\n",
    "- The dataset size decreases and the time to train the model also decreases\n",
    "\n",
    "- Removing stopwords can potentially help improve the performance as there are fewer and only meaningful tokens left. Thus, it could increase classification accuracy\n",
    "\n",
    "- Even search engines like Google remove stopwords for fast and relevant retrieval of data from the database\n",
    "\n",
    "### When Should we Remove Stopwords?\n",
    "\n",
    "#### Remove Stopwords\n",
    "We can remove stopwords while performing the following tasks:\n",
    "\n",
    "- Text Classification\n",
    "  - Spam Filtering\n",
    "  - Language Classification\n",
    "  - Genre Classification\n",
    "\n",
    "#### Avoid Stopword Removal\n",
    "- Machine Translation\n",
    "\n",
    "- Language Modeling\n",
    "\n",
    "- Text Summarization\n",
    "\n",
    "- Question-Answering problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stopword Removal using NLTK\n",
    "**NLTK has a list of stopwords stored in 16 different languages.** You can use the below code to see the list of English stopwords in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: {'are', 'so', 'with', 'any', 'ma', 've', 'yourself', \"couldn't\", 'am', 'the', 'where', 'such', 'will', \"you're\", 'hasn', 'that', 'mightn', 'wouldn', 's', 'than', 't', 'or', 'doesn', \"needn't\", 'weren', 'by', 'above', \"doesn't\", 'be', 'shouldn', 'he', 'been', \"wasn't\", 'most', 'we', 'now', 'couldn', 'me', 'won', 'too', 'haven', 'here', 'ourselves', 'once', 'theirs', 'below', 'myself', 'an', 'his', \"mightn't\", 'before', 'him', 'into', 'who', 'have', 'from', 'their', 'she', 'mustn', 'your', 'doing', 'all', 'off', 'were', 'then', 'while', 'over', 'very', \"she's\", 'these', 'll', 'not', 'up', 'themselves', 'how', 'when', 'y', 'out', 'what', 'there', \"didn't\", 'yourselves', 'which', 'our', 'itself', 'under', 'isn', \"it's\", 'each', 'was', 'does', 'about', \"hadn't\", 'if', 'down', 'of', 'you', 'being', 'herself', 'they', \"shouldn't\", 'my', 'd', 'her', 'whom', 'is', 'on', 'in', \"should've\", \"isn't\", 'its', 'needn', 'both', 'only', 'didn', 'again', 'having', \"mustn't\", 'a', 'nor', \"you've\", \"you'll\", \"aren't\", 'after', 'can', 'own', 'as', 'until', 'same', 'why', 'other', 'them', 'at', 'just', 'should', \"don't\", 'for', 'more', 'o', 're', \"haven't\", 'those', 'himself', 'do', 'did', 'don', 'has', 'yours', \"weren't\", 'to', 'against', 'but', 'during', \"that'll\", 'had', 'aren', 'i', \"wouldn't\", 'between', \"hasn't\", 'm', 'because', 'it', 'hers', 'few', \"shan't\", 'further', 'ours', 'no', \"won't\", 'shan', 'wasn', 'some', 'through', 'ain', 'and', \"you'd\", 'this', 'hadn'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stop words:\", stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs word tokenization on a text string using the `word_tokenize` function and then removes stop words, which are common words like \"is\" and \"with\" that are typically filtered out in text processing. The result is a list of significant words and punctuation from the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Filtered words: ['Hello', ',', 'world', '!', 'Natural', 'Language', 'Processing', 'Python', 'fun', '.', 'Let', \"'s\", 'learn', 'together', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"Filtered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stopword Removal using spaCy\n",
    "spaCy is one of the most versatile and widely used libraries in NLP. We can quickly and efficiently remove stopwords from the given text using SpaCy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation\n",
    "First, ensure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: download in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.3.5)\n",
      "Requirement already satisfied: en_core_web_sm in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from download) (1.16.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Filtered words: ['Hello', ',', 'world', '!', 'Natural', 'Language', 'Processing', 'Python', 'fun', '.', 'Let', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Loads the small English model in SpaCy, which contains vocabulary, syntax, and entities for English.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Processes the text and returns a `doc` object that contains the tokenized text and various annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterates over each token in the doc and includes the token in the filtered_words list if it is not a stop word (token.is_stop is False).\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Filtered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Stopword Removal using Gensim\n",
    "Gensim is a pretty handy library to work with on NLP tasks. While pre-processing, gensim provides methods to remove stopwords as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation\n",
    "First, ensure you have the necessary libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Filtered words: ['Hello,', 'world!', 'Natural', 'Language', 'Processing', 'Python', 'fun.', \"Let's\", 'learn', 'together.']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Remove stop words using Gensim\n",
    "filtered_text = remove_stopwords(text)\n",
    "filtered_words = filtered_text.split()\n",
    "\n",
    "print(\"Filtered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While using gensim for removing stopwords, we can directly use it on the raw text.** There’s no need to perform tokenization before removing stopwords. This can save us a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Normalization\n",
    "In any natural language, words can be written or spoken in more than one form depending on the situation. That’s what makes the language such a thrilling part of our lives, right? For example:\n",
    "\n",
    "- Lisa **ate** the food and washed the dishes.\n",
    "\n",
    "- They were **eating** noodles at a cafe.\n",
    "\n",
    "- Don’t you want to **eat** before we leave?\n",
    "\n",
    "- We have just **eaten** our breakfast.\n",
    "\n",
    "- It also **eats** fruit and vegetables.\n",
    "\n",
    "In all these sentences, we can see that the word **eat** has been used in multiple forms. For us, it is easy to understand that eating is the activity here. So it doesn’t really matter to us whether it is ‘ate’, ‘eat’, or ‘eaten’ – we know what is going on.\n",
    "\n",
    "Unfortunately, that is not the case with machines. They treat these words differently. Therefore, we need to normalize them to their root word, which is “eat” in our example.\n",
    "\n",
    "Hence, **text normalization** is a process of transforming a word into a single canonical form. This can be done by two processes, **stemming** and **lemmatization**. Let’s understand what they are in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stemming\n",
    "**Stemming** is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word. \n",
    "\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/nlp3.png\" alt=\"Alt text\" width=\"550\" height=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Stemmed Words: ['hello', ',', 'world', '!', 'natur', 'languag', 'process', 'python', 'fun', '.', 'let', \"'s\", 'learn', 'togeth', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Lemmatization\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/nlp4.png\" alt=\"Alt text\" width=\"550\" height=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! Natural Language Processing with Python is fun. Let's learn it together. \n",
      "\n",
      "Lemmatized Words: ['Hello', ',', 'world', '!', 'Natural', 'Language', 'Processing', 'Python', 'fun', '.', 'Let', \"'s\", 'learn', 'together', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Hello, world! Natural Language Processing with Python is fun. Let's learn it together.\"\n",
    "print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Why do we need to Perform Stemming or Lemmatization?\n",
    "Let’s consider the following two sentences:\n",
    "\n",
    "- He was driving\n",
    "\n",
    "- He went for a drive\n",
    "\n",
    "We can easily state that both the sentences are conveying the same meaning, that is, driving activity in the past. A machine will treat both sentences differently. Thus, to make the text understandable for the machine, we need to perform stemming or lemmatization.\n",
    "\n",
    "Another benefit of text normalization is that it reduces the number of unique words in the text data. This helps in bringing down the training time of the machine learning model (and don’t we all want that?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Stemming vs Lemmatization, which one should we prefer?\n",
    "**Stemming** algorithm works by cutting the suffix or prefix from the word. **Lemmatization** is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. **Hence, Lemmatization helps in forming better features.**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/nlp5.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting words to vectors\n",
    "In NLP, converting text into numerical vectors is essential for machine learning models to process and analyze textual data. Two common methods for this conversion:\n",
    "\n",
    "- The Bag of Words (BoW) model\n",
    "\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "These techniques transform text data into a format that can be fed into machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Bag of Words\n",
    "**Description**: The Bag of Words model represents text data as a matrix where each row corresponds to a document and each column corresponds to a word in the vocabulary. The value in each cell is the count of the occurrences of the word in the document.\n",
    "\n",
    "**Use Case**: The Bag of Words model is often used for text classification tasks, such as spam detection or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['explore' 'fascinating' 'is' 'language' 'learning' 'let' 'machine'\n",
      " 'makes' 'natural' 'nlp' 'powerful' 'processing' 'techniques' 'text' 'us'] \n",
      "\n",
      "Bag of Words Matrix:\n",
      " [[0 1 1 1 0 0 0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 0 0 1 1 1 1]] \n",
      "\n",
      "Bag of Words DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explore</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>let</th>\n",
       "      <th>machine</th>\n",
       "      <th>makes</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "      <th>techniques</th>\n",
       "      <th>text</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   explore  fascinating  is  language  learning  let  machine  makes  natural  \\\n",
       "0        0            1   1         1         0    0        0      0        1   \n",
       "1        0            0   0         0         1    0        1      1        0   \n",
       "2        1            0   0         0         0    1        0      0        0   \n",
       "\n",
       "   nlp  powerful  processing  techniques  text  us  \n",
       "0    0         0           1           0     0   0  \n",
       "1    1         1           0           0     0   0  \n",
       "2    0         0           1           1     1   1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'Natural Language Processing is fascinating.',\n",
    "    'Machine learning makes NLP powerful.',\n",
    "    'Let us explore text processing techniques.'\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into a document-term matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the matrix to an array\n",
    "X_array = X.toarray()\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame\n",
    "df_bow = pd.DataFrame(X_array, columns=feature_names)\n",
    "\n",
    "# Display the results\n",
    "print(\"Vocabulary:\", feature_names, \"\\n\")\n",
    "print(\"Bag of Words Matrix:\\n\", X_array, \"\\n\")\n",
    "print(\"Bag of Words DataFrame:\")\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TF-IDF\n",
    "**Description**: TF-IDF stands for Term Frequency-Inverse Document Frequency. It reflects the importance of a word in a document relative to the entire corpus. The TF-IDF score is high for words that are frequent in a document but not common across all documents, highlighting their importance.\n",
    "\n",
    "**Use Case**: TF-IDF is often used for information retrieval, document classification, and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['explore' 'fascinating' 'is' 'language' 'learning' 'let' 'machine'\n",
      " 'makes' 'natural' 'nlp' 'powerful' 'processing' 'techniques' 'text' 'us'] \n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.46735098 0.46735098 0.46735098 0.         0.\n",
      "  0.         0.         0.46735098 0.         0.         0.35543247\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.4472136  0.\n",
      "  0.4472136  0.4472136  0.         0.4472136  0.4472136  0.\n",
      "  0.         0.         0.        ]\n",
      " [0.42339448 0.         0.         0.         0.         0.42339448\n",
      "  0.         0.         0.         0.         0.         0.32200242\n",
      "  0.42339448 0.42339448 0.42339448]] \n",
      "\n",
      "TF-IDF DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explore</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>let</th>\n",
       "      <th>machine</th>\n",
       "      <th>makes</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>powerful</th>\n",
       "      <th>processing</th>\n",
       "      <th>techniques</th>\n",
       "      <th>text</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322002</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "      <td>0.423394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    explore  fascinating        is  language  learning       let   machine  \\\n",
       "0  0.000000     0.467351  0.467351  0.467351  0.000000  0.000000  0.000000   \n",
       "1  0.000000     0.000000  0.000000  0.000000  0.447214  0.000000  0.447214   \n",
       "2  0.423394     0.000000  0.000000  0.000000  0.000000  0.423394  0.000000   \n",
       "\n",
       "      makes   natural       nlp  powerful  processing  techniques      text  \\\n",
       "0  0.000000  0.467351  0.000000  0.000000    0.355432    0.000000  0.000000   \n",
       "1  0.447214  0.000000  0.447214  0.447214    0.000000    0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000    0.322002    0.423394  0.423394   \n",
       "\n",
       "         us  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.423394  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'Natural Language Processing is fascinating.',\n",
    "    'Machine learning makes NLP powerful.',\n",
    "    'Let us explore text processing techniques.'\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into a TF-IDF matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the matrix to an array\n",
    "X_tfidf_array = X_tfidf.toarray()\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame\n",
    "df_tfidf = pd.DataFrame(X_tfidf_array, columns=feature_names)\n",
    "\n",
    "# Display the results\n",
    "print(\"Vocabulary:\", feature_names, \"\\n\")\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf_array, \"\\n\")\n",
    "print(\"TF-IDF DataFrame:\")\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: Sentiment Analysis\n",
    "Sentiment analysis is the process of determining the emotional tone behind a body of text.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/nlp6.jpg\" alt=\"Alt text\" width=\"700\" height=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the sentiment behind the following example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis: {'neg': 0.0, 'neu': 0.298, 'pos': 0.702, 'compound': 0.9151}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "text = \"I love programming in Python! It's awesome and exciting.\"\n",
    "sentiment = sia.polarity_scores(text)\n",
    "print(\"Sentiment Analysis:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Explanation**:\n",
    "\n",
    "- `'neg'`: 0.0 - This indicates that there is no negative sentiment detected in the text.\n",
    "\n",
    "- `'neu'`: 0.298 - This shows that about 29.8% of the text is neutral.\n",
    "\n",
    "- `'pos'`: 0.702 - This indicates that about 70.2% of the text is positive.\n",
    "\n",
    "- `'compound'`: 0.9151 - The compound score is a normalized score ranging from -1 (most negative) to 1 (most positive). A score of 0.9151 suggests a very positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: Spam/Ham\n",
    "To build a Machine Learning model that can classify emails as spam or not spam using NLP techniques, we'll go through the following steps:\n",
    "\n",
    "1. Load and Preprocess Data\n",
    "\n",
    "2. Train-Test Split\n",
    "\n",
    "3. Feature Extraction\n",
    "\n",
    "4. Model Building\n",
    "\n",
    "5. Evaluation\n",
    "\n",
    "I'll use Python with libraries such as `scikit-learn`, `pandas`, and `nltk` for this example. We'll use a popular dataset for spam classification: the [SMS Spam Collection Dataset](https://archive.ics.uci.edu/dataset/228/sms+spam+collection). For simplicity, let’s assume the dataset is a CSV file with two columns: `label` (spam or ham) and `message` (the text of the email or SMS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Load and Preprocess Data\n",
    "First, we need to load the dataset and preprocess the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category  \\\n",
       "0      ham   \n",
       "1      ham   \n",
       "2     spam   \n",
       "3      ham   \n",
       "4      ham   \n",
       "\n",
       "                                                                                                                                                       Message  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the display option to show full content of each cell\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the extracted file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/machine-learning-content/master/assets/spam.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical to numerical transformation\n",
    "We transform our two categories `spam` and `ham` into numerical values (`0` and `1`) since this, like most models, does not work with categorical class variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category  \\\n",
       "0         0   \n",
       "1         0   \n",
       "2         1   \n",
       "3         0   \n",
       "4         0   \n",
       "\n",
       "                                                                                                                                                       Message  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to binary\n",
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elimination of repeated values\n",
    "We can easily count how many cases of each class we have to analyze whether the data set is balanced or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "Spam: 747\n",
      "No spam: 4825\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(f\"Spam: {len(df.loc[df.Category == 1])}\")\n",
    "print(f\"No spam: {len(df.loc[df.Category == 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates, if any, should also be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(inplace = False, drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we see that more than 400 repeated records have been eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "In order to train the model it is necessary to first apply a transformation process to the text. We start by transforming the text to lowercase and removing punctuation marks and special characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>until jurong point crazy vailable only in bugis great world la buffet ine there got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lar oking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ree entry in wkly comp to win up final tkts st ay ext to to receive entry question std txt rate apply over s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say so early hor already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ah don think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category  \\\n",
       "0         0   \n",
       "1         0   \n",
       "2         1   \n",
       "3         0   \n",
       "4         0   \n",
       "\n",
       "                                                                                                         Message  \n",
       "0                 until jurong point crazy vailable only in bugis great world la buffet ine there got amore wat   \n",
       "1                                                                                             lar oking wif oni   \n",
       "2   ree entry in wkly comp to win up final tkts st ay ext to to receive entry question std txt rate apply over s  \n",
       "3                                                                         dun say so early hor already then say   \n",
       "4                                                        ah don think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove any character that is not a letter (a-z) or white space ( )\n",
    "    text = re.sub(r'[^a-z ]', \" \", text)\n",
    "    \n",
    "    # Remove white spaces\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', \" \", text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', \" \", text)\n",
    "\n",
    "    # Multiple white spaces into one\n",
    "    text = re.sub(r'\\s+', \" \", text.lower())\n",
    "\n",
    "    # Remove tags\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"Message\"] = df[\"Message\"].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is lemmatization of the text, which is the process of simplifying words to their base or canonical form, so that words with different forms, but the same semantic core are treated as a single word.\n",
    "\n",
    "In addition, taking advantage of lemmatization, we will also eliminate stopwords, which are words that we consider irrelevant for text analysis because they appear very frequently in the language and do not provide meaningful information.\n",
    "\n",
    "Both tasks will be carried out with the Python library NLTK, which is one of the most important in terms of NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>jurong point crazy vailable bugis great world la buffet ine got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lar oking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ree entry wkly comp win final tkts st ay ext receive entry question std txt rate apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say early hor already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ah think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category  \\\n",
       "0         0   \n",
       "1         0   \n",
       "2         1   \n",
       "3         0   \n",
       "4         0   \n",
       "\n",
       "                                                                                  Message  \n",
       "0               jurong point crazy vailable bugis great world la buffet ine got amore wat  \n",
       "1                                                                       lar oking wif oni  \n",
       "2  ree entry wkly comp win final tkts st ay ext receive entry question std txt rate apply  \n",
       "3                                                           dun say early hor already say  \n",
       "4                                                      ah think go usf life around though  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "download(\"wordnet\")\n",
    "download(\"stopwords\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def lemmatize_text(message, lemmatizer = lemmatizer):\n",
    "    words = message.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"Message\"] = df[\"Message\"].apply(lemmatize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Message']\n",
    "y = df['Category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect `X_train_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaaaabe</th>\n",
       "      <th>aaaaabe</th>\n",
       "      <th>aaaat</th>\n",
       "      <th>aangalyam</th>\n",
       "      <th>aat</th>\n",
       "      <th>aathi</th>\n",
       "      <th>ab</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>ything</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>yuo</th>\n",
       "      <th>zed</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zzit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaaaa  aaaaaaaabe  aaaaabe  aaaat  aangalyam  aat  aathi   ab  \\\n",
       "0  0.0    0.0         0.0      0.0    0.0        0.0  0.0    0.0  0.0   \n",
       "1  0.0    0.0         0.0      0.0    0.0        0.0  0.0    0.0  0.0   \n",
       "2  0.0    0.0         0.0      0.0    0.0        0.0  0.0    0.0  0.0   \n",
       "3  0.0    0.0         0.0      0.0    0.0        0.0  0.0    0.0  0.0   \n",
       "4  0.0    0.0         0.0      0.0    0.0        0.0  0.0    0.0  0.0   \n",
       "\n",
       "   abdomen  ...  ything  yummy  yun  yuo  zed  zero  zhong  zoom  zyada  zzit  \n",
       "0      0.0  ...     0.0    0.0  0.0  0.0  0.0   0.0    0.0   0.0    0.0   0.0  \n",
       "1      0.0  ...     0.0    0.0  0.0  0.0  0.0   0.0    0.0   0.0    0.0   0.0  \n",
       "2      0.0  ...     0.0    0.0  0.0  0.0  0.0   0.0    0.0   0.0    0.0   0.0  \n",
       "3      0.0  ...     0.0    0.0  0.0  0.0  0.0   0.0    0.0   0.0    0.0   0.0  \n",
       "4      0.0  ...     0.0    0.0  0.0  0.0  0.0   0.0    0.0   0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 6463 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame\n",
    "pd.DataFrame(X_train_tfidf.toarray(), columns=feature_names).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Convert confusion matrix to a DataFrame for better visualization\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Ham', 'Actual Spam'], columns=['Predicted Ham', 'Predicted Spam'])\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False, linewidths=0.5)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
